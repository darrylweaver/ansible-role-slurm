# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
ControlMachine={{slurm_controller_name}}
#ControlAddr=
#BackupController=
#BackupAddr=
# 
AuthType=auth/munge
CacheGroups=0
#CheckpointType=checkpoint/blcr 
CryptoType=crypto/munge
#DisableRootJobs=NO 
#EnforcePartLimits=NO 
#Epilog=/usr/local/slurm/epilog_controller
#EpilogSlurmctld=/usr/local/slurm/epilog_controller
FirstJobId=10000 
MaxJobId=99999 
#GresTypes= 
#GroupUpdateForce=0 
#GroupUpdateTime=600 
#JobCheckpointDir=/var/slurm/checkpoint 
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0 
JobRequeue=0 
JobSubmitPlugins=all_partitions 
#KillOnBadExit=0 
Licenses={{slurm_licenses}}
#MailProg=/bin/mail 
#MaxJobCount=5000 
#MaxStepCount=40000 
#MaxTasksPerNode=128 
#MpiDefault=openmpi
#MpiParams=ports=12000-12999
#PluginDir= 
#PlugStackConfig= 
#PrivateData=jobs 
ProctrackType=proctrack/pgid
#Prolog=
#PrologSlurmctld= 
#PropagatePrioProcess=0 
#PropagateResourceLimits= 
#PropagateResourceLimitsExcept= 
ReturnToService=1
#SallocDefaultCommand= 

{% if ansible_distribution == "Ubuntu" and ansible_distribution_major_version == "16" %}
SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
SlurmctldLogFile=/var/log/slurm-llnl/slurmctld.log
SlurmdLogFile=/var/log/slurm-llnl/slurmd.log
SlurmdSpoolDir=/var/lib/slurm-llnl/slurmd
StateSaveLocation=/var/lib/slurm-llnl/checkpoint
{% else %}
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdLogFile=/var/log/slurm/slurmd.log
SlurmdSpoolDir=/var/spool/slurm
StateSaveLocation=/var/spool/slurm
{% endif %}

SlurmctldPort=6817
SlurmdPort=6818
SlurmUser=slurm
#SlurmdUser=root 
#SrunEpilog=
#SrunProlog=
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/none
#TaskPluginParam=
#TaskProlog=
#TopologyPlugin=topology/tree 
#TmpFs=/tmp 
#TrackWCKey=no 
#TreeWidth= 
#UnkillableStepProgram= 
#UsePAM=0 
# 
# 
# TIMERS 
#BatchStartTimeout=10 
#CompleteWait=0 
#EpilogMsgTime=2000 
#GetEnvTimeout=2 
#HealthCheckInterval=0 
#HealthCheckProgram= 
InactiveLimit=0
KillWait=30
MessageTimeout=30 
#ResvOverRun=0 
MinJobAge=300
#OverTimeLimit=0 
SlurmctldTimeout=30
SlurmdTimeout=40
#SlurmctldTimeout=120
#SlurmdTimeout=300
#UnkillableStepTimeout=60 
#VSizeFactor=0 
Waittime=0
# 
# 
# SCHEDULING 
#DefMemPerCPU=0 
FastSchedule=1
#MaxMemPerCPU=0 
#SchedulerRootFilter=1 
#SchedulerTimeSlice=30 
SchedulerType=sched/backfill
SchedulerPort=7321
SelectType=select/cons_res
SelectTypeParameters=CR_CPU
# 
# 
# JOB PRIORITY 
#PriorityType=priority/basic 
#PriorityDecayHalfLife= 
#PriorityCalcPeriod= 
#PriorityFavorSmall= 
#PriorityMaxAge= 
#PriorityUsageResetPeriod= 
#PriorityWeightAge= 
#PriorityWeightFairshare= 
#PriorityWeightJobSize= 
#PriorityWeightPartition= 
#PriorityWeightQOS= 
# 
# 
# LOGGING AND ACCOUNTING 
#AccountingStorageEnforce=0 
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/none
#AccountingStorageUser=
AccountingStoreJobComment=YES
ClusterName=cluster
DebugFlags=NO_CONF_HASH
#DebugFlags= 
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
SlurmctldDebug=4
SlurmdDebug=4
#SlurmSchedLogFile= 
#SlurmSchedLogLevel= 
# 
# 
# POWER SAVE SUPPORT FOR IDLE NODES (optional) 
#SuspendProgram= 
#ResumeProgram= 
#SuspendTimeout= 
#ResumeTimeout= 
#ResumeRate= 
#SuspendExcNodes= 
#SuspendExcParts= 
#SuspendRate= 
#SuspendTime= 
# 
# 
# COMPUTE NODES 
{% if slurm_worker_nodenames %}
NodeName={{ slurm_worker_nodenames | join(",") }} CPUs={{ slurm_worker_cpus }} Sockets={{ slurm_worker_sockets }} CoresPerSocket={{ slurm_worker_cores_per_socket }} ThreadsPerCore={{ slurm_worker_threads_per_core }}
PartitionName={{slurm_partition_name}} Nodes={{ slurm_worker_nodenames | join(",") }} Default=YES MaxTime=INFINITE State=UP
{% elif slurm_worker_ips %}
NodeName={{slurm_vnode_prefix}}[1-{{slurm_worker_ips|length}}] CPUs={{ slurm_worker_cpus }} Sockets={{ slurm_worker_sockets }} CoresPerSocket={{ slurm_worker_cores_per_socket }} ThreadsPerCore={{ slurm_worker_threads_per_core }}
PartitionName={{slurm_partition_name}} Nodes={{slurm_vnode_prefix}}[1-{{slurm_worker_ips|length}}] Default=YES MaxTime=INFINITE State=UP
{% else %}
NodeName=localhost CPUs=1
PartitionName={{slurm_partition_name}} Nodes=localhost Default=YES MaxTime=INFINITE State=UP
{% endif %}
